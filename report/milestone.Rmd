---
title: "Capstone Project"
subtitle: "Milestone Report"
author: "Brian Gulbis"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
    html_document:
        toc: true
        toc_float: true
        code_fold: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

The goal of this project is to build a predictive text model, using the HC Corpora data set. The data are in English and come from three different types of websites: blogs, news, or Twitter. In this milestone report, the data were loaded into `r R.Version()$version.string` and an exploratory analysis was performed.

* Most common word in all three data sets is `the`
* Followed by `and`, `to`, `a`, `of` in blogs and news; tweets differs
* Most common word-pairs: `in the` (all 3), `of the` (blogs and news)
* Most common 3-gram: `one of the`, `a lot of` (blogs and news)
* Blogs and News have a lot of similarities; Twitter differs

## Introduction

This is the final project for the Data Science Capstone course. The goal of this project is to build a predictive text model, such as those used by smart keyboards on mobile devices. The model will be built using the [HC Corpora](http://www.corpora.heliohost.org/aboutcorpus.html) data set. The data contained in the HC Corpora were obtained from publicly available websites using a web crawler. The data were then divided into three categories based on the type of website: blogs, news, or Twitter. The project will focus on building a predictive text model for the English language. 

The goal of this milestone report was to explore the three data sets in the HC Corpora. The data were processed using `r R.Version()$version.string`. Due to the size of the data sets, a random sample of 20% of the total data was used to perform most of the exploratory analysis. 

The data do contain some words which are considered profanity which should not be included in the prediction model. A [list of profane words](https://gist.github.com/jamiew/1112488) compiled by Google was used to identify words for exclusion from the data set.

Throughout the report, click on the `Code` button to view the raw R code.

## Exploratory Analysis

The exploratory analysis utilized the R package `tidytext` to perform text mining, along with the `tidyverse` packages for data manipulation.

```{r, message=FALSE}
library(tidyverse)
library(tidytext)
library(stringr)
library(forcats)
library(wordcloud)
```

```{r functions}
make_tokens <- function(df) {
    df %>%
        unnest_tokens(word, value) %>%
        anti_join(profanity, by = c("word" = "value")) %>%
        filter(!str_detect(word, "[[:digit:]]"))
}

filter_instances <- function(df, perc, val) {
    df %>%
        filter(run_total <= perc * val)
}

make_ngram <- function(df, n) {
    sep_cols <- c("first", "second")
    if (n == 3) sep_cols <- c(sep_cols, "third")
    
    x <- df %>%
        unnest_tokens(word, value, token = "ngrams", n = n, collapse = FALSE) %>%
        separate(word, sep_cols, sep = " ", remove = FALSE) %>%
        anti_join(profanity, by = c("first" = "value")) %>%
        anti_join(profanity, by = c("second" = "value")) 
    
    if (n == 3) x <- anti_join(x, profanity, by = c("third" = "value"))
    
    filter(x, !str_detect(word, "[[:digit:]]"))
}
```


```{r}
profanity <- read_lines("../data/external/profanity.txt") %>%
    as_tibble()

blogs <- read_lines("../data/raw/en_US.blogs.txt.gz") %>%
    as_tibble() %>%
    mutate(num_char = nchar(value))

news <- read_lines("../data/raw/en_US.news.txt.gz") %>%
    as_tibble() %>%
    mutate(num_char = nchar(value))

tweets <- read_lines("../data/raw/en_US.twitter.txt.gz") %>%
    as_tibble() %>%
    mutate(num_char = nchar(value))
```

### Summary of Data Sets

The following table provides a basic overview of the three data sets contained in the HC Corpora, including the number of lines of data in each set, and summary statistics for the number of characters per line in each set. Unsurprising, the blogs and news data sets have a much larger mean number of characters per line compared with the Twitter set, where tweets are limited to maximum of 140 characters.

```{r}
num_char <- bind_rows(tidy(summary(blogs$num_char)),
               tidy(summary(news$num_char)),
               tidy(summary(tweets$num_char)))

df <- data_frame(`Data Set` = c("blogs", "news", "tweets"), 
                 `Lines #` = c(nrow(blogs), nrow(news), nrow(tweets))) %>%
    bind_cols(num_char) %>%
    rename(`Min # Characters` = minimum,
           `First Quantile` = q1,
           `Median # Characters` = median,
           `Mean # Characters` = mean,
           `Third Quantile` = q3,
           `Max # Characters` = maximum)

knitr::kable(df, format.args = list(big.mark = ","))
```

### Exploring Word Counts

The next table provides a summary of the number of word instances contained in each set, as well as the number of unique words. It should be noted that prior to calculating the number of word instances, words considered profanity as well as words containing numeric digits were removed.

```{r}
blogs_tokens <- make_tokens(blogs["value"])
news_tokens <- make_tokens(news["value"])
tweets_tokens <- make_tokens(tweets["value"])

blogs_unique <- count(blogs_tokens, word, sort = TRUE)
news_unique <-  count(news_tokens, word, sort = TRUE)
tweets_unique <-  count(tweets_tokens, word, sort = TRUE)

df <- data_frame(`Data Set` = c("blogs", "news", "tweets"), 
                 `Word Instances` = c(nrow(blogs_tokens), nrow(news_tokens), nrow(tweets_tokens)),
                 `Unique Words` = c(nrow(blogs_unique), nrow(news_unique), nrow(tweets_unique)))

knitr::kable(df, format.args = list(big.mark = ","))
```

In each set, the number of words which make up 50% and 90% of the total word instances was calculated. This demonstrates that a small number of words make up the majority of word instances. 

```{r}
blogs_sum_words <-  mutate(blogs_unique, run_total = cumsum(n))
blogs_50p <- filter_instances(blogs_sum_words, 0.5, nrow(blogs_tokens))
blogs_90p <- filter_instances(blogs_sum_words, 0.9, nrow(blogs_tokens))

news_sum_words <-  mutate(news_unique, run_total = cumsum(n))
news_50p <- filter_instances(news_sum_words, 0.5, nrow(news_tokens))
news_90p <- filter_instances(news_sum_words, 0.9, nrow(news_tokens))

tweets_sum_words <-  mutate(tweets_unique, run_total = cumsum(n))
tweets_50p <- filter_instances(tweets_sum_words, 0.5, nrow(tweets_tokens))
tweets_90p <- filter_instances(tweets_sum_words, 0.9, nrow(tweets_tokens))

df <- data_frame(`Data Set` = c("blogs", "news", "tweets"),
                 `Words in 50% of Instances` = c(nrow(blogs_50p), nrow(news_50p), nrow(tweets_50p)),
                 `Words in 90% of Instances` = c(nrow(blogs_90p), nrow(news_90p), nrow(tweets_90p)))

knitr::kable(df, format.args = list(big.mark = ","))
```

```{r}
rm(blogs_tokens, news_tokens, tweets_tokens)

frac <- 0.2

set.seed(77123)
blogs_sample <- sample_frac(blogs["value"], frac) 
news_sample <- sample_frac(news["value"], frac) 
tweets_sample <-  sample_frac(tweets["value"], frac) 
```

```{r}
blogs_tokens_sample <- make_tokens(blogs_sample)
news_tokens_sample <- make_tokens(news_sample)
tweets_tokens_sample <- make_tokens(tweets_sample)
```

```{r}
blogs_sample_unique <- count(blogs_tokens_sample, word, sort = TRUE)
news_sample_unique <- count(news_tokens_sample, word, sort = TRUE)
tweets_sample_unique <- count(tweets_tokens_sample, word, sort = TRUE)
```

Using a sample of 20% of the total data, we can visualize the top 250 unique words in each data set as a word cloud. Words in the word cloud are sized based on their frequency within the data set.

```{r}
blogs_sample_unique %>%
    with(wordcloud(word, n, max.words = 250))
```

We can further explore the top 25 most frequent words in a bar graph, which allows us to more accurately compare the relative frequency of each word. 
```{r}
blogs_sample_unique %>%
    top_n(25, n) %>%
    dmap_at("word", fct_inorder) %>%
    ggplot(aes(x = word, y = n)) +
    geom_bar(stat = "identity") +
    ggtitle("Frequency of Top 25 Words in the Blogs Data Set")
```

```{r}
news_sample_unique %>%
    with(wordcloud(word, n, max.words = 250))
```

```{r}
news_sample_unique %>%
    top_n(25, n) %>%
    dmap_at("word", fct_inorder) %>%
    ggplot(aes(x = word, y = n)) +
    geom_bar(stat = "identity") +
    ggtitle("Frequency of Top 25 Words in the News Data Set")
```

```{r}
tweets_sample_unique %>%
    with(wordcloud(word, n, max.words = 250))
```

```{r}
tweets_sample_unique %>%
    top_n(25, n) %>%
    dmap_at("word", fct_inorder) %>%
    ggplot(aes(x = word, y = n)) +
    geom_bar(stat = "identity") +
    ggtitle("Frequency of Top 25 Words in the Twitter Data Set")
```

### N-Gram Phrases

Next, 2-word pairs and 3-word phrases are created using the n-gram method. These will be used in the prediction model to predict what the next word will be based upon the current word being typed. 

```{r}
rm(blogs, news, tweets)

blogs_2gram <- make_ngram(blogs_sample, 2)
blogs_3gram <- make_ngram(blogs_sample, 3)

news_2gram <- make_ngram(news_sample, 2)
news_3gram <- make_ngram(news_sample, 3)

tweets_2gram <- make_ngram(tweets_sample, 2)
tweets_3gram <- make_ngram(tweets_sample, 3)
```

```{r}
blogs_2gram_unique <- count(blogs_2gram, word, sort = TRUE)
blogs_3gram_unique <- count(blogs_3gram, word, sort = TRUE)

news_2gram_unique <- count(news_2gram, word, sort = TRUE)
news_3gram_unique <- count(news_3gram, word, sort = TRUE)

tweets_2gram_unique <- count(tweets_2gram, word, sort = TRUE)
tweets_3gram_unique <- count(tweets_3gram, word, sort = TRUE)
```

A summary of the number of the word-pair/word-phrase instances and unique word-pairs/word-phrases contained in the data sample is presented in the table below.

```{r}
df <- data_frame(`Data Set` = c("blogs", "news", "tweets"),
                 `Two-Word Pairs` = c(nrow(blogs_2gram), nrow(news_2gram), nrow(tweets_2gram)),
                 `Unique Two-Word Pairs` = c(nrow(blogs_2gram_unique), nrow(news_2gram_unique), nrow(tweets_2gram_unique)),
                 `Three-Word Phrases` = c(nrow(blogs_3gram), nrow(news_3gram), nrow(tweets_3gram)),
                 `Unique Three-Word Phrases` = c(nrow(blogs_3gram_unique), nrow(news_3gram_unique), nrow(tweets_3gram_unique)))

knitr::kable(df, format.args = list(big.mark = ","))
```

The frequency of the top 25 2-word pairs in each data set are displayed below. Interestingly, the top 4 word-pairs are the same in the blogs and news sets, while the Twitter set contains similar phrases but in a different order.

```{r}
blogs_2gram_unique %>%
    top_n(25, n) %>%
    dmap_at("word", fct_inorder) %>%
    ggplot(aes(x = word, y = n)) +
    geom_bar(stat = "identity") +
    ggtitle("Frequency of Top 25 2-Word Pairs in Blogs Data Set") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
news_2gram_unique %>%
    top_n(25, n) %>%
    dmap_at("word", fct_inorder) %>%
    ggplot(aes(x = word, y = n)) +
    geom_bar(stat = "identity") +
    ggtitle("Frequency of Top 25 2-Word Pairs in News Data Set") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
tweets_2gram_unique %>%
    top_n(25, n) %>%
    dmap_at("word", fct_inorder) %>%
    ggplot(aes(x = word, y = n)) +
    geom_bar(stat = "identity") +
    ggtitle("Frequency of Top 25 2-Word Pairs in Twitter Data Set") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The top 25 three-word phrases in each set are presented below. The top 3 phrases are the same in the blogs and news sets, however, the top phrases in the Twitter set vary considerably.

```{r}
blogs_3gram_unique %>%
    top_n(25, n) %>%
    dmap_at("word", fct_inorder) %>%
    ggplot(aes(x = word, y = n)) +
    geom_bar(stat = "identity") +
    ggtitle("Frequency of Top 25 3-Word Phrases in Blogs Data Set") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
news_3gram_unique %>%
    top_n(25, n) %>%
    dmap_at("word", fct_inorder) %>%
    ggplot(aes(x = word, y = n)) +
    geom_bar(stat = "identity") +
    ggtitle("Frequency of Top 25 3-Word Phrases in News Data Set") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
tweets_3gram_unique %>%
    top_n(25, n) %>%
    dmap_at("word", fct_inorder) %>%
    ggplot(aes(x = word, y = n)) +
    geom_bar(stat = "identity") +
    ggtitle("Frequency of Top 25 3-Word Phrases in Twitter Data Set") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Prediction Algorithm Design

* Synsets, hypernyms to find related words
